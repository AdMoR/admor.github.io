---
description: Essential ressources to use it for fun and profit
tags: NLP transformers text generation
---

# Why tranformers ?

Published in 2018, the transformer architecture was a breakthrought for NLP tasks, providing massive uplift compared to previous models.

However even if the paper ![Attention is all you need](https://arxiv.org/pdf/1706.03762.pdf) is well written, it takes time to understand all the subtilities of the model.

This post is my personal cheatsheet for this kind of models.


### Understanding the architecture

Rather than repetiting something that has already been done better, here are the links for understanding the model that I found the most helpful.

- [The illustrated Transformer](https://jalammar.github.io/illustrated-transformer/) : great article with insightful visualization. Other NLP articles are also available on the same blog.
- [The annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html) : Detailled implementation of the model, step by step. Really insightful to understand the details of the architecture


### Getting started to experiment

I would recommend using HuggingFace great repository on Github to play with this kid of architecture as they have all the state of the art models

[HuggingFace's Transformers](https://github.com/huggingface/transformers)

They support both Pytorch and Tensorflow, so everyone gets what he needs.


