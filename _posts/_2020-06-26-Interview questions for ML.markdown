---
description: Summary of 
tags: ML  
---

### Maximum likelihood estimation

$$ max L(\theta | X ) = max L(x | \theta) \frac{ P(\theta)}{ P(X) }  $$

P over X and theta are uniform sop we can ignore themm as they don't affect the maximum.


### Second order optimizer 

The update rule is the following

$$ \theta_{t+1} = \theta_{t} + H^{-1} \nabla f(t) $$

H is rthe hessian 
$$ \Nabla $$ is the gradient 

Limitations : 

LBFGS --> explanation of how the Hessian is computed 

Gauss Newton --> 

limitation when N samples < N dimensonality

### Definition of convex problem

...


### Derivations of functions

Sigmoid : $$  $$

Softmax : $$ $$

### References : 

Wikipedia : 

